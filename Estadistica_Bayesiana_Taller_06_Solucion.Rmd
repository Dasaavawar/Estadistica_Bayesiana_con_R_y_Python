---
title: "Taller 6"
author: 
- Webpage https://sites.google.com/view/juansosa/ 
- YouTube https://www.youtube.com/c/JuanSosa1702 
- GitHub  https://github.com/jstats1702 
- Rpubs   https://rpubs.com/jstats1702
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newcommand{\lf}{\left}
\newcommand{\rg}{\right}
\newcommand{\ra}{\sqrt}
\newcommand{\xv}{\boldsymbol{x}}
\newcommand{\yv}{\boldsymbol{y}}
\newcommand{\tev}{\boldsymbol{\theta}}
\newcommand{\ga}{\gamma}
\newcommand{\ro}{\rho}
\newcommand{\ep}{\epsilon}
\newcommand{\lam}{\lambda}
\newcommand{\ups}{\upsilon}
\newcommand{\te}{\theta}
\newcommand{\si}{\sigma}
\newcommand{\ka}{\kappa}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\xb}{\bar{x}}
\newcommand{\yt}{y_t}
\newcommand{\ytm}{y_{t-1}}
\newcommand{\G}{\Gamma}
\newcommand{\pro}{\propto}
\newcommand{\ent}{\Rightarrow}
\newcommand{\caja}{\boxed}
\newcommand{\var}[1]{\mathbb{V}ar\left(#1\right)}
\newcommand{\esp}[1]{\mathbb{E}\left(#1\right)}
\newcommand{\ex}[1]{ \exp{ \left\{ #1 \right\} } }

1. Una línea de producción se basa en mediciones precisas de un algoritmo de reconocimiento de imágenes en la primera etapa del proceso. Se sabe que el algoritmo es insesgado, así que es razonable asumir que las mediciones siguen una distribución normal con media cero, $y_i\mid\sigma^2\stackrel{\text{iid}}{\sim}\textsf{N}(0,\sigma^2)$, para $i=1,\ldots,n$. Algunos errores están permitidos, pero si se excede un umbral $c$, entonces el algoritmo se debe reemplazar. Se hacen $n=20$ mediciones y se observa que $\sum_{i=1}^n y_i = -2$ y $\sum_{i=1}^n y_i^2 = 15$. Asumiendo que $\sigma^2\sim\textsf{GI}(a,b)$, calcule la probabilidad posterior de que $\sigma>c$ en las siguientes instancias:

- $c=1$ y $a=b=0.1$.
- $c=1$ y $a=b=10$.
- $c=2$ y $a=b=0.1$.
- $c=2$ y $a=b=10$.

La distribución posterior es tal que:
$$
p(\sigma^2\mid\boldsymbol{y}) \propto \prod_{i=1}^n\textsf{N}(y_i\mid 0,\sigma^2)\times \textsf{GI}(\sigma^2\mid a,b) \propto (\sigma^2)^{-n/2}\,\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n y_i^2\right\}\times (\sigma^2)^{-(a+1)}\,\exp\left\{-\frac{b}{\sigma^2}\right\}
$$
donde $\boldsymbol{y}=(y_1,\ldots,y_n)$. Por lo tanto,
$$
p(\sigma^2\mid\boldsymbol{y}) \propto (\sigma^2)^{-(a + n/2 + 1)}\,\exp\left\{-\frac{1}{\sigma^2}\left(b + \frac{1}{2}\sum_{i=1}^n y_i^2 \right)\right\}
$$
lo cual corresponde al núcleo de una distribución Gamma Inversa con parámetros $a+n/2$ y $b+\frac12\sum_{i=1}^n y_i^2$. Por lo tanto, $\sigma^2\mid\boldsymbol{y}\sim\textsf{GI}(a+n/2,b+\frac12\sum_{i=1}^n y_i^2)$.

Entonces, la probabilidad posterior de que $\sigma>c$ está dada por:
```{r}
# estadisticos
n   <- 20
sy  <- -2
sy2 <- 15
# caso 1
tab <- rbind(
  c(0.1, 0.1, 1, mean(sqrt(1/rgamma(n = 10000, shape = 0.1, rate = 0.1)) > 1)),
  c(10 , 10 , 1, mean(sqrt(1/rgamma(n = 10000, shape = 10 , rate = 10 )) > 1)),
  c(0.1, 0.1, 2, mean(sqrt(1/rgamma(n = 10000, shape = 0.1, rate = 0.1)) > 2)),
  c(10 , 10 , 2, mean(sqrt(1/rgamma(n = 10000, shape = 10 , rate = 10 )) > 2)))
colnames(tab) <- c("a","b","c", "P(sigma > c)")
knitr::kable(x = tab, digits = 3, align = "c")
```


2. Sea $y_i\mid\theta \stackrel{\text{iid}}{\sim}\textsf{N}(\theta,1)$, para $i=1,\ldots,n$. Considere una distribución previa impropia para $\theta$ de la forma $p(\theta)=1$ para todo $\theta$ (una distribución previa de esta forma se denomina impropia porque $\int_{\Theta} p (\theta)\,\textsf{d}\theta$ diverge). Halle la distribución posterior de $\theta$ y muestre que es una función de densidad propia.

En este caso la distribución posterior es:
$$
p(\theta\mid\boldsymbol{y}) \propto \prod_{i=1}^n\textsf{N}(y_i\mid\theta,1) \propto \exp\left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\theta)^2 \right\} 
\propto \exp\left\{ -\frac{1}{2}\left[ \theta^2\frac{n}{\sigma^2} - 2\theta\,\frac{n}{\sigma^2}\bar{y} \right]\right\}
$$
donde $\boldsymbol{y}=(y_1,\ldots,y_n)$. Por lo tanto,
$$
p(\theta\mid\boldsymbol{y})
\propto \exp\left\{ -\frac{1}{2}\left[ \theta^2\frac{n}{\sigma^2} - 2\theta\,\frac{n}{\sigma^2}\bar{y} \right]\right\}
$$
lo cual corresponde al núcleo de una distribución Normal con media $\bar{y}=\frac{1}{n}\sum_{i=1}^n y_i$ y varianza $1/(n/\sigma^2)=\sigma^2/n$. Por lo tanto, $\theta\mid\boldsymbol{y}\sim\textsf{N}(\bar{y},\sigma^2/n)$, lo cual corresponde a una distribución de probabilidad propia.

3. Considere el modelo $y_i\mid\sigma^2_i\stackrel{\text{iid}}{\sim}\textsf{N}(0,\sigma^2_i)$, para $i=1,\ldots,n$, con $\sigma^2_i\mid\beta\sim\textsf{G}(4,\beta)$ y $\beta\sim\textsf{G}(1,1)$.

a. Representar el modelo por medio de un DAG.

```{r, eval = TRUE, echo=FALSE, out.width="20%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("taller-06-DAG3.jpg")
```

b. Hallar las distribuciones condicionales completas de $\sigma_i$ y $\beta$.

- Distribución condicional completa de $\sigma^2_i$:
$$
p(\sigma^2_i\mid\text{resto}) \propto \textsf{N}(y_i\mid0,\sigma^2_i) \times \textsf{GI}(\sigma^2\mid 4,\beta) \propto (\sigma_i^2)^{-1/2}\exp\left\{-\frac{1}{2\sigma_i^2}\,y_i^2\right\}\times (\sigma_i^2)^{-(4+1)}\,\exp\left\{-\frac{\beta}{\sigma_i^2}\right\}\,.
$$
Por lo tanto $\sigma^2_i\mid\text{resto}\sim\textsf{GI}(4+\frac12,\beta + y_i^2/2)$.

- Distribución condicional completa de $\beta$:
$$
p(\beta\mid\text{resto}) \propto \prod_{i=1}^n \textsf{GI}(\sigma_i^2\mid 4,\beta) \times \textsf{G}(\beta\mid 1, 1) \propto \beta^{4n}\,\exp\left\{-\beta\sum_{i=1}^n\frac{1}{\sigma_i^2}\right\} \times \beta^{1-1}\,\exp\left\{-\beta\right\}\,.
$$
Por lo tanto $\beta\mid\text{resto}\sim \textsf{G}(1+4n,1+\sum_{i=1}^n1/\sigma_i^2)$.

c. Implementar un muestreador de Gibbs para obtener muestras de la distribución posterior de $(\sigma^2_1,\ldots,\sigma^2_n,\beta)$, asumiendo que $n=10$ y que $y_i=i$ para todo $i$.

```{r}
# datos
n <- 10
y <- 1:10
# inicializar muestreador
set.seed(1)
bet  <- rgamma(n = 1, shape = 1, rate = 1)
sig2 <- 1/rgamma(n = n, shape = 4, rate = bet)
# muestreador de Gibbs
B <- 10000
THETA <- matrix(data = NA, nrow = B, ncol = n+1)
LP <- NULL
set.seed(1)
for (b in 1:B) {
  sig2 <- 1/rgamma(n = n, shape = 4.5, rate = bet + 0.5*y^2)
  bet  <- rgamma(n = 1, shape = 1+4*n, rate = 1 + sum(1/sig2))
  THETA[b,] <- c(sig2, bet)
  LP[b] <- sum(dnorm(x = y, mean = 0, sd = sqrt(sig2), log = T))
}
# fin del algoritmo
```

d. Chequear exhaustivamente la convergencia del algoritmo.

Cadena de la log-verosimilitud:

```{r}
plot(x = 1:B, y = LP, type = "l", xlab = "Iteración", ylab = "Log-verosimilitud", main = "Log-verosimilitud")
```
Cadena de $\beta$:

```{r}
plot(x = 1:B, y = THETA[,n+1], type = "l", xlab = "Iteración", ylab = expression(beta), main = expression(beta))
```
Cadena de un $\sigma^2_i$ elegido aleatoriamente:

```{r}
set.seed(1)
i <- sample(x = 1:n, size = 1)
i
plot(x = 1:B, y = THETA[,i], type = "l", xlab = "Iteración", ylab = expression(sigma[i]^2), main = expression(sigma[i]^2))
```
Tamaños efectivos de muestra:

```{r}
neff <- summary(coda::effectiveSize(THETA))
neff
```
Errores estándar de Monte Carlo:

```{r}
summary(apply(X = THETA, MARGIN = 2, FUN = sd)/coda::effectiveSize(THETA))
```
No hay evidencia de que la cadena no haya alcanzado la distribución estacionaria. Además, los tamaños efectivos de muestra son los suficientemente grandes para realizar inferencia estadística sobre los parámetros del modelo (el error estándar de Monte Carlo más grande es de 0.00123).

e. Hacer inferencia posterior sobre los parámetros del modelo. 

Inferencia sobre $\sigma^2_1,\ldots,\sigma^2_n$:

```{r, fig.width=10, fig.height=5}
ids  <- 1:n
that <- colMeans(THETA[,1:n])
ic1  <- apply(X = THETA[,1:n], MARGIN = 2, FUN = function(x) quantile(x, c(0.025,0.975)))
# grafico
plot(NA, NA, xlab = "Departamento", ylab = expression(sigma[i]^2), main = expression(sigma[i]^2), xlim = c(1,n), ylim = range(ic1), cex.axis = 0.75, xaxt = "n")
axis(side = 1, at = 1:n, labels = 1:n)
abline(v = 1:n, col = "gray95", lwd = 1, lty = 3)
for (j in 1:n) {
  segments(x0 = j, y0 = ic1[1,j], x1 = j, y1 = ic1[2,j], lwd = 1)
  lines(x = j, y = that[j], type = "p", pch = 16, cex = 0.8)
}
```

Inferencia sobre $\beta$:

```{r, fig.width=12, fig.height=4}
# beta
plot(density(THETA[,1]), xlab = expression(beta), main = expression(beta), lwd = 2, ylab = expression(beta), xlim = c(0,15))
abline(v = quantile(THETA[,1], c(.025,.5,.975)), col=c(2,4,2), lty=c(3,2,3))
legend("top", legend = c("Media", "IC"), fill = c(4,2), col = c(4,2), border = c(4,2), bty = "n")
```


4. Considere el número de hijos de hombres de 30 años con (`menchild30bach.dat`) y sin (`menchild30nobach.dat`) títulos de pregrado. Sean $\theta_A$ y $\theta_B$ la tasa promedio del número de hijos de hombres de 30 años con y sin pregrado, respectivamente. Se asumen distribuciones muestrales de Poisson para los dos grupos, de forma que $\theta_A = \theta$ y $\theta_B=\gamma\theta$. En esta parametrización, $\gamma$ representa la razón de tasas $\theta_B/\theta_A$. Finalmente, se asume que $\theta$ y $\gamma$ son independientes a prior con $\theta\sim\textsf{G}(a_\theta,b_\theta)$ y $\gamma\sim\textsf{G}(a_\gamma,b_\gamma)$.

a. Representar el modelo por medio de un DAG.

```{r, eval = TRUE, echo=FALSE, out.width="30%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("taller-06-DAG4.jpg")
```
b. ¿Los parámetros $\theta_A$ y $\theta_B$ son independientes?

$\theta_A$ y $\theta_B$ no son independientes. La estructura probabilística de $\theta_B$ depende directamente de la estructura probabilística de $\theta_A$ dado que $\theta_B$ es función de $\theta_A$. Para hacer 

c. Hallar la distribución condicional completa de $\theta$.

La distribución condicional completa de $\theta$ es:
$$
p(\theta\mid\text{resto}) \propto \prod_{i=1}^{n_A} \textsf{Poi}(y_{A,i}\mid\theta) \times \prod_{i=1}^{n_B} \textsf{Poi}(y_{B,i}\mid\gamma\theta)\times \textsf{G}(\theta\mid a_\theta,b_\theta)
\propto e^{-n_A\theta}\theta^{\sum_{i=1}^{n_A} y_{A,i}} \times e^{-n_B\gamma\theta}\theta^{\sum_{i=1}^{n_B} y_{B,i}}\times \theta^{a_\theta-1}\,e^{-b_\theta\theta}\,.
$$
Por lo tanto, $\theta\mid\text{resto}\sim\textsf{G}(a_\theta + \sum_{i=1}^{n_A} y_{A,i} + \sum_{i=1}^{n_B} y_{B,i}, b_\theta + n_A + n_B\gamma)$.

d. Hallar la distribución condicional completa de $\gamma$.

La distribución condicional completa de $\gamma$ es:
$$
p(\gamma\mid\text{resto}) \propto \prod_{i=1}^{n_B} \textsf{Poi}(y_{B,i}\mid\gamma\theta)\times \textsf{G}(\gamma\mid a_\gamma,b_\gamma)
\propto e^{-n_B\gamma\theta}\gamma^{\sum_{i=1}^{n_B} y_{B,i}}\times \gamma^{a_\gamma-1}\,e^{-b_\gamma\gamma}\,.
$$
Por lo tanto, $\gamma\mid\text{resto}\sim\textsf{G}(a_\gamma + \sum_{i=1}^{n_B} y_{B,i}, b_\gamma + n_B\theta)$.

e. Hallar el valor esperado previo y el coeficiente de variación previo de $\theta$ y graficar la distribución previa de $\theta$ con $a_\theta = 2$ y $b_\theta=1$.

A priori se tiene que:
$$
\textsf{E}(\theta) = 2\,,
\qquad
\textsf{Var}(\theta) = 2\,,
\qquad
\textsf{CV}(\theta) = \frac{\sqrt{2}}{2}\approx70.7\%\,.
$$
Gráfico de la distribución previa de $\theta$ con $a_\theta = 2$ y $b_\theta=1$:

```{r}
curve(expr = dgamma(x, shape = 2, rate = 2), from = 0, to = 5, n = 1000, xlab = expression(theta), ylab = "Densidad", main = "", col = "royalblue")
```



f. Hallar el valor esperado previo y el coeficiente de variación previo de $\gamma$  y graficar la distribución previa de $\gamma$ con $a_\gamma=b_\gamma\in\{1; 2; 4; 8; 16; 32; 64; 128\}$.

Si $a_\gamma=b_\gamma=a$, entonces a priori se tiene que:
$$
\textsf{E}(\gamma) = 1\,,
\qquad
\textsf{Var}(\theta) = 1/a\,,
\qquad
\textsf{CV}(\theta) = \frac{\sqrt{a}}{a}\,.
$$
Por lo tanto, para  $a\in\{1; 2; 4; 8; 16; 32; 64; 128\}$:

```{r}
as <- 2^(0:7)
as
tab <- cbind(1,1/as,100*sqrt(as)/as)
colnames(tab) <- c("E","SD","CV (%)")
knitr::kable(x = tab, digits = 2, align = "c")
```

Gráfico de la distribución previa de $\gamma$ con con $a_\gamma=b_\gamma\in\{1; 2; 4; 8; 16; 32; 64; 128\}$.

```{r}
col = RColorBrewer::brewer.pal(n = 8, name = "Set1")
curve(expr = dgamma(x, shape = 128, rate = 128), from = 0, to = 4, n = 1000, xlab = expression(theta), ylab = "Densidad", main = "", col = col[8])
for (i in 7:1)
  curve(expr = dgamma(x, shape = as[i], rate = as[i]), col = col[i], add = T)
legend("topright", bty = "n", legend = paste("a = ", as), col = col, lwd = 2)
```


g. Usando todos los valores de los hiperparámetros dados anteriormente, implementar un muestreador de Gibbs con al menos $B=1000$ iteraciones (después de un periodo de calentamiento de 1000 iteraciones), y chequear exhaustivamente la convergencia del algoritmo.

```{r}
# datos
# yA: sí pregrado
# yB: no pregrado
yA <- c(1,0,0,1,2,2,1,5,2,0,0,0,0,0,0,1,1,1,0,0,0,1,1,2,1,3,2,0,0,3,0,0,0,2,1,0,2,
        1,0,0,1,3,0,1,1,0,2,0,0,2,2,1,3,0,0,0,1,1)
yB <- c(2,2,1,1,2,2,1,2,1,0,2,1,1,2,0,2,2,0,2,1,0,0,3,6,1,6,4,0,3,2,0,1,0,0,0,3,0,
        0,0,0,0,1,0,4,2,1,0,0,1,0,3,2,5,0,1,1,2,1,2,1,2,0,0,0,2,1,0,2,0,2,4,1,1,1,
        2,0,1,1,1,1,0,2,3,2,0,2,1,3,1,3,2,2,3,2,0,0,0,1,0,0,0,1,2,0,3,3,0,1,2,2,2,
        0,6,0,0,0,2,0,1,1,1,3,3,2,1,1,0,1,0,0,2,0,2,0,1,0,2,0,0,2,2,4,1,2,3,2,0,0,
        0,1,0,0,1,5,2,1,3,2,0,2,1,1,3,0,5,0,0,2,4,3,4,0,0,0,0,0,0,2,2,0,0,2,0,0,1,
        1,0,2,1,3,3,2,2,0,0,2,3,2,4,3,3,4,0,3,0,1,0,1,2,3,4,1,2,6,2,1,2,2)
# estadisticos
nA <- length(yA)
nB <- length(yB)
sA <- sum(yA)
sB <- sum(yB)
# hiperparametros
a_t <- 2
b_t <- 1
a_g <- 2^(0:7)
# algoritmo
nburn <- 1000
nsams <- 10000
B     <- nburn + nsams
THETA <- array(data = NA, dim = c(B,2,8))
LP    <- array(data = NA, dim = c(B,8))
set.seed(1)
for (i in 1:length(a_g)) {
  # inicializar
  a <- a_g[i]
  theta <- 1
  gamma <- 1
  # muestreador
  for (b in 1:B) {
    # actualizar
    theta <- rgamma(n = 1, shape = a_t + sA + sB, rate = b_t + nA + nB*gamma)
    gamma <- rgamma(n = 1, shape = a + sB, rate = a + nB*theta)
    # almacenar
    THETA[b,,i] <- c(theta, gamma)
    # log verosimilitud
    LP[b,i] <- sum(dpois(x = yA, lambda = theta, log = T)) + sum(dpois(x = yB, lambda = gamma*theta))
  }
}
# fin del algoritmo
```

Cadenas de la log-verosimilitud (removiendo el periodo de calentamiento):

```{r}
col = RColorBrewer::brewer.pal(n = 8, name = "Set1")
plot(x = NA, y = NA, xlim = c(1,nsams), ylim = range(LP), xlab = "Iteración", ylab = "Log-verosimilitud")
for (i in 1:ncol(LP)) lines(x = 1:nsams, y = LP[-(1:nburn),i], col = adjustcolor(col = col[i], alpha.f = 0.2))
```
Tamaños efectos de muestra (recomiendo el periodo de calentamiento):

```{r}
tmp <- NULL
for (i in 1:ncol(LP))
  tmp <- rbind(tmp, c(a_t, b_t, a_g[i], coda::effectiveSize(THETA[-(1:nburn),,i])))
colnames(tmp) <- c("a_theta","b_theta","a_gamma","theta","gamma")
knitr::kable(x = tmp, digits = 2, align = "c")
```

Errores estándar de Monte Carlo:

```{r}
tmp <- NULL
for (i in 1:ncol(LP))
  tmp <- rbind(tmp, c(a_t, b_t, a_g[i], apply(X = THETA[-(1:nburn),,i], MARGIN = 2, FUN = sd)/coda::effectiveSize(THETA[-(1:nburn),,i])))
colnames(tmp) <- c("a_theta","b_theta","a_gamma","theta","gamma")
knitr::kable(x = tmp, digits = 5, align = "c")
```

No hay evidencia de que la cadena no haya alcanzado la distribución estacionaria. Además, los tamaños efectivos de muestra son los suficientemente grandes para realizar inferencia estadística sobre los parámetros del modelo (el error estándar de Monte Carlo más grande es de 0.00025).

h. Hallar e interpretar en cada caso la media posterior de $\theta_B-\theta_A$, el coeficiente de variación posterior de $\theta_B-\theta_A$, el intervalo de credibilidad al 95\% para $\theta_B-\theta_A$, la probabilidad posterior de $\theta_B>\theta_A$, y la probabilidad posterior de $\tilde{y}_B>\tilde{y}_A$, donde $\tilde{y}$ denota una observación de la distribución predictiva posterior, y probar el sistema de hipótesis $H_0:\gamma=1$ frente a $H_1:\gamma\neq 1$. Describir el efecto de la especificación previa sobre los resultados a posteriori.

A partir de las muestras de Monte Carlo se puede calcular fácilmente la media posterior de $\theta_B-\theta_A$, el coeficiente de variación posterior de $\theta_B-\theta_A$, el intervalo de credibilidad al 95\% para $\theta_B-\theta_A$, la probabilidad posterior de $\theta_B>\theta_A$, y la probabilidad posterior de $\tilde{y}_B>\tilde{y}_A$.

De otra parte, para probar el sistema de hipótesis $H_0:\gamma=\gamma_0$ frente a $H_1:\gamma\neq\gamma_0$, con $\gamma_0=1$, a continuación se presenta el cálculo del factor de Bayes $B_{10}$. Asumiendo que $\textsf{Pr}(H_0)=\textsf{Pr}(H_1)=0.5$, el factor de Bayes $B_{01}$ asociado con este sistema de hipótesis es:
$$
B_{01} = \frac{p(\boldsymbol{y}_A,\boldsymbol{y}_B\mid H_0)}{p(\boldsymbol{y}_A,\boldsymbol{y}_B\mid H_1)}\,.
$$

Así,
$$
\begin{align*}
p(\boldsymbol{y}_A,\boldsymbol{y}_B\mid H_0) &= \int_{0}^\infty\int_{0}^\infty p(\boldsymbol{y}_A,\boldsymbol{y}_B\mid\theta,\gamma,H_0)\,p(\theta,\lambda\mid H_0)\,\text{d}\theta\,\text{d}\gamma \\
&= \int_{0}^\infty\int_{0}^\infty p(\boldsymbol{y}_A\mid\theta,H_0)\,p(\boldsymbol{y}_B\mid\theta,\gamma,H_0)\,p(\theta\mid H_0)\,p(\gamma\mid H_0)\,\text{d}\theta\,\text{d}\gamma \\
&= \int_{0}^\infty\int_{0}^\infty \left[\frac{e^{-n_A\theta}\theta^{s_A}}{\prod_{i=1}^{n_A} y_{A,i}!}\right] \, \left[\frac{e^{-n_B\theta}\theta^{s_B}}{\prod_{i=1}^{n_B} y_{B,i}!}\right] \, \left[\frac{b_\theta^{a_\theta}}{\Gamma(a_\theta)}\theta^{a_\theta-1} e^{-b_\theta\theta}\right] \,\delta_{1}(\gamma)\,\text{d}\theta\,\text{d}\gamma \\
&= \frac{1}{\prod_{i=1}^{n_A} y_{A,i}!}\,\frac{1}{\prod_{i=1}^{n_B} y_{B,i}!}\,\frac{b_\theta^{a_\theta}}{\Gamma(a_\theta)}
\int_{0}^\infty \theta^{a_\theta + s_A + s_B - 1}\, e^{-(b_\theta + n_A + n_B)\theta} \,\text{d}\theta\ \\
&= \frac{1}{\prod_{i=1}^{n_A} y_{A,i}!}\,\frac{1}{\prod_{i=1}^{n_B} y_{B,i}!}\,\frac{b_\theta^{a_\theta}}{\Gamma(a_\theta)}  
\, \frac{\Gamma(a_\theta + s_A + s_B )}{(b_\theta + n_A + n_B)^{a_\theta + s_A + s_B }}
\end{align*}
$$
donde $s_A = \sum_{i=1}^{n_A} y_{A,i}$ y $s_B = \sum_{i=1}^{n_B} y_{B,i}$, y $\delta_a(x)$ es la función delta de Dirac en el punto $a$, dado que $\int_{0}^\infty\delta_{1}(\gamma)\,\text{d}\gamma = 1$.  
	
Además,
$$
\begin{align*}
p(\boldsymbol{y}_A,\boldsymbol{y}_B\mid H_1) &= \int_{0}^\infty\int_{0}^\infty p(\boldsymbol{y}_A,\boldsymbol{y}_B\mid\theta,\gamma,H_1)\,p(\theta,\lambda\mid H_1)\,\text{d}\theta\,\text{d}\gamma \\
&= \int_{0}^\infty\int_{0}^\infty p(\boldsymbol{y}_A\mid\theta,H_1)\,p(\boldsymbol{y}_B\mid\theta,\gamma,H_1)\,p(\theta\mid H_1)\,p(\gamma\mid H_1)\,\text{d}\theta\,\text{d}\gamma \\
&= \int_{0}^\infty\int_{0}^\infty \left[\frac{e^{-n_A\theta}\theta^{s_A}}{\prod_{i=1}^{n_A} y_{A,i}!}\right] \, \left[\frac{e^{-n_B\gamma\theta}(\gamma\theta)^{s_B}}{\prod_{i=1}^{n_B} y_{B,i}!}\right] \, \left[\frac{b_\theta^{a_\theta}}{\Gamma(a_\theta)}\theta^{a_\theta-1} e^{-b_\theta\theta}\right] \,
\left[\frac{b_\gamma^{a_\gamma}}{\Gamma(a_\gamma)}\gamma^{a_\gamma-1} e^{-b_\gamma\gamma}\right] \,\text{d}\theta\,\text{d}\gamma \\
&= \frac{1}{\prod_{i=1}^{n_A} y_{A,i}!}\,\frac{1}{\prod_{i=1}^{n_B} y_{B,i}!}\,\,\frac{b_\theta^{a_\theta}}{\Gamma(a_\theta)} \,\frac{b_\gamma^{a_\gamma}}{\Gamma(a_\gamma)}
\int_{0}^\infty\int_{0}^\infty \theta^{a_\theta + s_A + s_B - 1}\, e^{-(b_\theta + n_A + n_B\gamma)\theta}\,\gamma^{a_\gamma + s_B - 1}\,e^{-b_\gamma\gamma} \,\text{d}\theta\, \text{d}\gamma\\
&= \frac{1}{\prod_{i=1}^{n_A} y_{A,i}!}\,\frac{1}{\prod_{i=1}^{n_B} y_{B,i}!}\,\frac{b_\theta^{a_\theta}}{\Gamma(a_\theta)} \,\frac{b_\gamma^{a_\gamma}}{\Gamma(a_\gamma)}
\int_{0}^\infty \gamma^{a_\gamma + s_B - 1}\,e^{-b_\gamma\gamma} \int_{0}^\infty \theta^{a_\theta + s_A + s_B - 1}\, e^{-(b_\theta + n_A + n_B\gamma)\theta} \,\text{d}\theta\, 
\text{d}\gamma\\
&= \frac{1}{\prod_{i=1}^{n_A} y_{A,i}!}\,\frac{1}{\prod_{i=1}^{n_B} y_{B,i}!}\,\frac{b_\theta^{a_\theta}}{\Gamma(a_\theta)} \,\frac{b_\gamma^{a_\gamma}}{\Gamma(a_\gamma)}
\int_{0}^\infty \gamma^{a_\gamma + s_B - 1}\,e^{-b_\gamma\gamma}\,\frac{\Gamma(a_\theta + s_A + s_B)}{(b_\theta + n_A + n_B\gamma)^{a_\theta + s_A + s_B}}\,  \text{d}\gamma\\
&= \frac{1}{\prod_{i=1}^{n_A} y_{A,i}!}\,\frac{1}{\prod_{i=1}^{n_B} y_{B,i}!}\,\frac{b_\theta^{a_\theta}}{\Gamma(a_\theta)} \,\frac{b_\gamma^{a_\gamma}}{\Gamma(a_\gamma)}\,
\Gamma(a_\theta + s_A + s_B)\,\int_{0}^\infty \frac{1}{(b_\theta + n_A + n_B\gamma)^{a_\theta + s_A + s_B}}\,\gamma^{a_\gamma + s_B - 1}\,e^{-b_\gamma\gamma}\,  \text{d}\gamma\\
&= \frac{1}{\prod_{i=1}^{n_A} y_{A,i}!}\,\frac{1}{\prod_{i=1}^{n_B} y_{B,i}!}\,\frac{b_\theta^{a_\theta}}{\Gamma(a_\theta)}\,\frac{b_\gamma^{a_\gamma}}{\Gamma(a_\gamma)}\,\Gamma(a_\theta + s_A + s_B)\,\frac{\Gamma(a_\gamma+s_B)}{b_\gamma^{a_\gamma+s_B}}
\int_{0}^\infty (b_\theta + n_A + n_B\gamma)^{-(a_\theta + s_A + s_B)}\,\frac{b_\gamma^{a_\gamma+s_B}}{\Gamma(a_\gamma+s_B)}\,\gamma^{a_\gamma + s_B - 1}\,e^{-b_\gamma\gamma}\,  \text{d}\gamma\\
&= \frac{1}{\prod_{i=1}^{n_A} y_{A,i}!}\,\frac{1}{\prod_{i=1}^{n_B} y_{B,i}!}\,\frac{b_\theta^{a_\theta}}{\Gamma(a_\theta)}\,\frac{b_\gamma^{a_\gamma}}{\Gamma(a_\gamma)}\,\Gamma(a_\theta + s_A + s_B)\,\frac{\Gamma(a_\gamma+s_B)}{b_\gamma^{a_\gamma+s_B}}
\textsf{E}_{\gamma}\left((b_\theta + n_A + n_B\gamma)^{-(a_\theta + s_A + s_B)}\right)
\end{align*}
$$
donde $\textsf{E}_{\gamma}(g(\gamma))$ es el valor esperado de $g(\gamma) = (b_\theta + n_A + n_B\gamma)^{-(a_\theta + s_A + s_B)}$ bajo $\gamma\sim\textsf{G}(a_\gamma+s_B,b_\gamma)$, el cual se puede calcular fácilmente usando muestras iid de $\textsf{G}(a_\gamma+s_B,b_\gamma)$.

Por lo tanto,
$$
B_{01} = \frac{\frac{1}{\prod_{i=1}^{n_A} y_{A,i}!}\,\frac{1}{\prod_{i=1}^{n_B} y_{B,i}!}\,\frac{b_\theta^{a_\theta}}{\Gamma(a_\theta)}\,\frac{\Gamma(a_\theta + s_A + s_B )}{(b_\theta + n_A + n_B)^{a_\theta + s_A + s_B }}}
{\frac{1}{\prod_{i=1}^{n_A} y_{A,i}!}\,\frac{1}{\prod_{i=1}^{n_B} y_{B,i}!}\,\frac{b_\theta^{a_\theta}}{\Gamma(a_\theta)}\,\frac{b_\gamma^{a_\gamma}}{\Gamma(a_\gamma)}\,\Gamma(a_\theta + s_A + s_B)\,\frac{\Gamma(a_\gamma+s_B)}{b_\gamma^{a_\gamma+s_B}}
\textsf{E}_{\gamma}\left((b_\theta + n_A + n_B\gamma)^{-(a_\theta + s_A + s_B)}\right)}
=
\frac{b_\gamma^{s_B}}{1}\,\frac{\Gamma(a_\gamma)}{\Gamma(a_\gamma+s_B)}\,\frac{\,(b_\theta + n_A + n_B)^{-(a_\theta + s_A + s_B)}}{\textsf{E}_{\gamma}\left((b_\theta + n_A + n_B\gamma)^{-(a_\theta + s_A + s_B)}\right)}\,,
$$
de donde
$$
B_{10} 
=
\frac{1}{b_\gamma^{s_B}}\,\frac{\Gamma(a_\gamma+s_B)}{\Gamma(a_\gamma)}\,\frac{\textsf{E}_{\gamma}\left((b_\theta + n_A + n_B\gamma)^{-(a_\theta + s_A + s_B)}\right)}{\,(b_\theta + n_A + n_B)^{-(a_\theta + s_A + s_B)}}\,,
$$
o alternativamente, 
$$
 B_{10} = \exp\left(
 -s_B\log b_\gamma + \log\Gamma(a_\gamma+s_B) - \log\Gamma(a_\gamma) + \log\textsf{E}_{\gamma}\left((b_\theta + n_A + n_B\gamma)^{-(a_\theta + s_A + s_B)}\right) + (a_\theta + s_A + s_B)\log(b_\theta + n_A + n_B)
\right)\,,
$$
con
$$
\textsf{E}_{\gamma}\left((b_\theta + n_A + n_B\gamma)^{-(a_\theta + s_A + s_B)}\right) = 
\int_{0}^\infty (b_\theta + n_A + n_B\gamma)^{-(a_\theta + s_A + s_B)}\,\frac{b_\gamma^{a_\gamma+s_B}}{\Gamma(a_\gamma+s_B)}\,\gamma^{a_\gamma + s_B - 1}\,e^{-b_\gamma\gamma}\,  \text{d}\gamma
\approx \frac{1}{B}\sum_{b=1}^B \left(b_\theta + n_A + n_B\gamma^{(b)}\right)^{-(a_\theta + s_A + s_B)}\,,
$$
con $\gamma^{(1)},\ldots,\gamma^{(B)}\stackrel{\text{iid}}{\sim}\textsf{G}(a_\gamma+s_B,b_\gamma)$. 

```{r}
# inferencia
# eta = theta_B - theta_A = theta*(gamma - 1) 
tab <- NULL
for (i in 1:ncol(LP)) {
  thetaA <- THETA[-(1:nburn),1,i]
  thetaB <- THETA[-(1:nburn),1,i]*THETA[-(1:nburn),2,i]
  yrepA  <- rpois(n = B, lambda = thetaA)
  yrepB  <- rpois(n = B, lambda = thetaB)
  # factor de bayes
  gMC <- rgamma(n = nsams, shape = a_g[i] + sB, rate = a_g[i])
  B10 <- exp(-sB*log(a_g[i]) + lgamma(a_g[i] + sB) - lgamma(a_g[i]) + mean(exp(-(a_t + sA + sB)*log(b_t + nA + nB*gMC))) + (a_t + sA + sB)*log(b_t + nA + nB)) 
  # salida
  tab <- rbind(tab, c(a_t, b_t, a_g[i], 
                      mean(thetaB - thetaA), 
                      100*abs(sd(thetaB - thetaA)/mean(thetaB - thetaA)), 
                      quantile(thetaB - thetaA, c(.025, .975)), 
                      mean(thetaB - thetaA > 0), 
                      mean(yrepB - yrepA > 0),
                      B10))
}
colnames(tab) <- c("a_theta","b_theta","a_gamma","Media","CV (%)","Q2.5%","Q97.5%","P(theta_B > theta_A)","P(y*_B > y*_A)","B_10")
knitr::kable(x = tab, digits = 3, align = "c")
```

Así, tanto los intervalos de credibilidad de $\theta_B-\theta_A$ como las probabilidades posteriores de $\theta_B - \theta_A > 0$ y los factores de Bayes, indican que la tasa de número de hijos de los hombres sin pregrado es significativamente superior de la tasa del número de hijos de los hombres con pregrado. Bajo todas las formulaciones previas tal evidencia es absolutamente decisiva, pero se hace menos fuerte a medida que el grado de concentración de $\theta_B$ a priori al rededor de $1$ es más fuerte.


5. Sea $y_t=\rho y_{t-1}+\epsilon_t$, con $\epsilon_t\mid\sigma^2\stackrel{\text{iid}}{\sim}\textsf{N}(0,\sigma^2)$, para $t=1,\ldots,n$, asumiendo una distribución previa impropia de la forma $\pi(\rho,\sigma^2)\propto 1/\sigma^2$. Este es un modelo popular en el análisis de series temporales conocido como modelo autorregresivo de orden uno o AR(1).

a. Hallar la verosimilitud condicional en $y_1$, i.e., $f(y_2,\ldots,y_n\mid y_1,\rho,\sigma^2)$.

Sea $\tev\equiv(\ro,\si^2)$. Así, se tiene que
$$
\begin{align*}
f(y_{2:n}|\,y_1, \tev )
&=\frac{f(y_2,\ldots,y_n|\,y_1,\tev)}{f(y_2,\ldots,y_{n-1}|\,y_1,\tev)}\cdot
\frac{f(y_2,\ldots,y_{n-1}|\,y_1,\tev)}{f(y_2,\ldots,y_{n-2}|\,y_1,\tev)}\\
&\hspace{3cm}\cdots\cdot
\frac{f(y_2,y_3,y_4|\,y_1,\tev)}{f(y_2,y_3|\,y_1,\tev)}\cdot
\frac{f(y_2,y_3|\,y_1,\tev)}{f(y_2|\,y_1,\tev)}\cdot f(y_2|\,y_1,\tev)\\
&=f(y_n|\,y_1,\ldots,y_{n-1},\tev)\cdot
f(y_{n-1}|\,y_1,\ldots,y_{n-2},\tev)\,\\
&\hspace{3cm}\cdots\cdot f(y_4|\,y_1,y_2,y_3,\tev)\cdot
f(y_3|\,y_1,y_2,\tev)\cdot f(y_2|\,y_1,\tev)\\
&=f(y_n|y_{n-1},\tev)\,
f(y_{n-1}|y_{n-2},\tev)\cdots f(y_4|y_3,\tev) f(y_3|y_2,\tev)f(y_2|\,y_1,\tev)\\
\end{align*}
$$
dado que
$$
f(y_{t}|\,y_1,\ldots,\ytm,\tev) = f(y_{t}|y_{t-1},\tev)\,,\,\,\,t=2,\ldots,n
$$
Entonces, la verosimilitud condicional dado $y_1$ está dada por
$$
\caja{
f(y_2,\ldots,y_n|\,y_1, \ro,\si^2)=\prod_{t=2}^n f(y_t|\,\ytm,\ro,\si^2)}
$$
Además, como
$$
\begin{align*}
        &\esp{y_t|\,\ytm,\tev} = \esp{\ro\ytm + \ep_t|\,\ytm,\tev}=\ro\ytm\,,\\
        &\var{y_t|\,\ytm,\tev} = \var{\ro\ytm + \ep_t|\,\ytm,\tev}=\si^2\,,\\
        &\ep_t\sim \textsf{N}(0,\si^2)\,,
\end{align*}
$$
se sique que $(y_t\mid\,\ytm,\tev)\sim \textsf{N}(\ro\ytm,\si^2)$.

b. Hallar la distribución posterior basada en la verosimilitud condicional. 

Sea $\yv\equiv(y_1,\ldots,y_n)$. Asumiendo que la distribución previa es de la forma $\pi(\ro,\si^2)\pro 1/\si^2$, se tiene que la distribución posterior está dada por:

\begin{align*}
p(\tev|\,\yv)\pro f(\yv|\,\tev)\,p(\tev) =f(y_2,\ldots,y_n|\,y_1,\tev)\,f(y_1|\tev)\,p(\tev).
\end{align*}
Teniendo en cuenta que se puede usar la verosimilitud condicional $f(y_{2:n}|\,y_1,\tev)$ como una aproximación de la verosimilitud, se obtiene la distribución posterior,
\begin{align*}
p(\tev|\,\yv)&\pro f(y_2,\ldots,y_n|\,y_1,\tev)\,p(\tev)\\
    &\pro \frac{1}{\si^2}\,\prod_{t=2}^n f(y_t|\,\ytm,\ro,\si^2)\\
    &\pro \frac{1}{\si^2}\,\prod_{t=2}^n \frac{1}{\ra{2\pi}\si}\,\ex{ -\frac{1}{2\si^2}(y_t - \ro\ytm)^2 }\\
    &\pro \frac{1}{\si^2}\,\frac{1}{\si^{n-1}}\,\ex{ -\frac{1}{2\si^2}\sum_{t=2}^n(y_t - \ro\ytm)^2 }\\
    &\pro \frac{1}{\si^{n+1}}\,\ex{ -\frac{1}{2\si^2}\lf[\sum_{t=2}^n y_t^2 - 2\ro\sum_{t=2}^n y_t\ytm + \ro^2\sum_{t=2}^n \ytm^2\rg] }\\
    &\pro \frac{1}{\si^{n+1}}\,\ex{ -\frac{1}{2}\lf[ \ro^2\frac{\sum_{t=2}^n \ytm^2}{\si^2} - 2\ro\frac{\sum_{t=2}^n y_t\ytm}{\si^2} \rg] }\,\ex{ -\frac{\sum_{t=2}^n y_t^2}{2\si^2} }.
\end{align*}

Definiendo
\begin{align*}
        &\frac{1}{\tau^2}\equiv\frac{\sum_{t=2}^n \ytm^2}{\si^2}
        \quad\ent\quad
        \caja{\tau^2=\frac{\si^2}{\sum_{t=2}^n \ytm^2}}\\
        &\frac{\mu}{\tau^2}\equiv\frac{\sum_{t=2}^n y_t\ytm}{\si^2}
        \quad\ent\quad
        \caja{\mu=\frac{\sum_{t=2}^n y_t\ytm}{\sum_{t=2}^n \ytm^2}}
\end{align*}
se sigue que
\begin{align*}
        p(\tev|\,\yv)&\pro
        \frac{1}{\si^{n+1}}\,
        \ex{ -\frac{1}{2}\lf[ \frac{\ro^2}{\tau^2}- 2\ro\frac{\mu}{\tau^2} \rg] }\,\ex{ -\frac{\sum_{t=2}^n y_t^2}{2\si^2} }\\
        &\pro
        \frac{1}{\si^{n+1}}\,
        \ex{ -\frac{1}{2\tau^2}(\ro - \mu)^2 }\,\ex{ -\frac{\sum_{t=2}^n y_t^2}{2\si^2} + \frac{\mu^2}{2\tau^2} }.
\end{align*}
Pero,
$$
    \frac{\mu^2}{\tau^2}=\frac{\sum_{t=2}^n \ytm^2}{\si^2}\,\frac{\lf[\sum_{t=2}^n y_t\ytm\rg]^2}{\lf[\sum_{t=2}^n \ytm^2\rg]^2}=
    \frac{\lf[\sum_{t=2}^n y_t\ytm\rg]^2}{\si^2\sum_{t=2}^n \ytm^2}
$$
lo que significa que
\begin{align*}
        p(\tev|\,\yv)
        &\pro
        \frac{1}{\si^{n+1}}\,
        \ex{ -\frac{1}{2\tau^2}(\ro - \mu)^2 }\,
        \ex{ -\frac{1}{\si^2}\lf[\frac{\sum_{t=2}^n y_t^2}{2} - \frac{\lf[\sum_{t=2}^n y_t\ytm\rg]^2}{2\sum_{t=2}^n \ytm^2} \rg]}\\
        &\pro
        \frac{1}{\si^{n+1}}\,
        \ex{ -\frac{1}{2\tau^2}(\ro - \mu)^2 }\,
        \ex{ -\frac{\be}{\si^2}}
\end{align*}
donde
$$
\caja{
\be=\frac{\lf[\sum_{t=2}^n y_t^2\rg]\,\lf[ \sum_{t=2}^n \ytm^2 \rg]- \lf[\sum_{t=2}^n y_t\ytm\rg]^2}{2\sum_{t=2}^n \ytm^2}}
$$
Entonces,
\begin{align*}
        p(\tev|\,\yv)
        &\pro
        \frac{1}{\tau}\,
        \ex{ -\frac{1}{2\tau^2}(\ro - \mu)^2 }\,
        \frac{\tau}{\si^{n+1}}\,
        \ex{ -\frac{\be}{\si^2}}\\
                &\pro
        \frac{1}{\tau}\,
        \ex{ -\frac{1}{2\tau^2}(\ro - \mu)^2 }\,
        \frac{\si}{\ra{\sum_{t=2}^n \ytm^2}}\,\frac{1}{\si^{n+1}}\,
        \ex{ -\frac{\be}{\si^2}}\\
                    &\pro
        \frac{1}{\tau}\,
        \ex{ -\frac{1}{2\tau^2}(\ro - \mu)^2 }\,
        \lf(\si^2\rg)^{-(\al + 1)}\,
        \ex{ -\frac{\be}{\si^2}}
\end{align*}
donde
$$
\caja{
\al=\frac{n}{2} - 1}
$$
Normalizando, se obtiene que
\begin{align*}
\caja{
p(\tev|\,\yv)=
        \frac{1}{\ra{2\pi}\,\tau}\,
        \ex{ -\frac{1}{2\tau^2}(\ro - \mu)^2 }\,\cdot
        \frac{\be^\al}{\G(\al)}\,\lf(\si^2\rg)^{-(\al + 1)}\,
        \ex{ -\frac{\be}{\si^2}}}
\end{align*}
lo que significa que
$$
    p(\tev|\,\yv)=\pi_1(\ro|\,\si^2, \yv)\,\pi_2(\si^2|\,\yv)
$$
con $\pi_1(\ro|\,\si^2, \yv)=N(\mu,\tau^2)$ y $\pi_2(\si^2|\,\yv)=IG(\al,\be)$.

c. Hallar $p(\rho\mid\sigma^2,y_1,\ldots,y_n)$ y $p(\sigma^2\mid y_1,\ldots,y_n)$.

Teniendo en cuenta esta distribución posterior en mente, se sigue directamente que
$$
\begin{align*}
        &p(\si^2|\, \yv)= \int p(\tev|\,\yv)\,d\ro=\int \pi_1(\ro|\,\si^2, \yv)\,\pi_2(\si^2|\,\yv)\,d\ro=\pi_2(\si^2|\,\yv)\\
        &\ent \quad \caja{p(\si^2|\, \yv)=\pi_2(\si^2|\,\yv)}\quad\therefore\quad\caja{(\si^2|\, \yv) \sim IG(\al,\be)}
\end{align*}
$$

Además,
\begin{align*}
        &p(\ro|\,\si^2, \yv)= \frac{p(\ro,\si^2|\,\yv)}{p(\si^2|\, \yv)}=\frac{\pi_1(\ro|\,\si^2, \yv)\,\pi_2(\si^2|\,\yv)}{\pi_2(\si^2|\,\yv)}=\pi_1(\ro|\,\si^2, \yv)\\
        &\ent \quad \caja{p(\ro|\,\si^2, \yv)=\pi_1(\ro|\,\si^2, \yv)}\quad\therefore\quad\caja{(\ro|\,\si^2, \yv) \sim N\lf(\mu,\tau^2\rg)}
\end{align*}

d. Simular dos conjuntos de datos con $n=500$, uno con $(\rho,\sigma^2)=(0.95,4)$ y el otro con $(\rho,\sigma^2)=(0.3,4)$. Ajustar el modelo y realizar la inferencia posterior correspondiente en cada caso.

Se simulan dos conjuntos de datos de acuerdo con los siguientes casos:

- Caso 1: $n=500$, $\ro=0.95$, y $\si^2=4$.
- Caso 2: $n=500$, $\ro=0.3$, y $\si^2=4$.

Además, la primera observación $y_1$ en cada caso se genera de acuerdo con una distribución Normal con media 0 y varianza $\si^2/(1-\ro^2)$. Así, se lleva a cabo una simulación de Monte Carlo con $M=5,000$ iteraciones para obtener muestras de la distribución posterior, por medio de las distribuciones $(\si^2|\,\yv)$ y $(\rho|\,\si^2,\yv)$. Además, la distribución posterior empírica se compara con las expresiones análiticas para confirmar los resultados. Finalmente, la siguiente tabla muestra los resumenes de las distribuciones posteriores correspondientes.

```{r}
### data: funcion para generar y ###
data <- function(n, rho, sigma) {
	y <- rep(NA, n)
	v <- sqrt( sigma^2/(1 - rho^2) )
	y[1] <- rnorm(n=1, mean=0, sd=v)
	eps  <- rnorm(n=n, mean=0, sd=sigma)
	for(i in 2:n) {
		y[i] <- rho*y[i-1] + eps[i]
	}
	y
}
```

```{r}
###---'-----###
# Escenario 1 #
###----'----###
### parametros iniciales ###
n <- 500
rho <- 0.95
sigma <- 2 
time <- 1:n
###--###
# Data #
###--###
set.seed(123456789)
y <- data(n, rho, sigma)
plot(time, y, type="l", xlab="t", ylab=expression(y[t]), las=1, cex.axis=0.75, main="Caso 1", sub = ' ')
```

```{r}
##-'-------------------###
# distribucion posterior #
###--------------------###
### sumA := sum_{t=2}^n y_t y_{t-1}^2 ###
sumA <- sum( y[-c(1)] * y[-c(n)] )
### sumB := sum_{t=2}^n y_{t-1}^2 ###
sumB <- sum( (y[-c(n)])^2 )
### sumC := sum_{t=2}^n y_t^2 ###
sumC <- sum( (y[-c(1)])^2 )
### parametros posterior ###
alpha <- n/2 - 1
beta <- ( sumB * sumC - sumA^2 ) / ( 2 * sumB )
### post.sigma := p(sigma^2 | y) ###
suppressMessages(suppressWarnings(require(pscl)))
post.sigma <- function(x){
	### aproximacion funcion gamma ###
	temp1 <- - ( (alpha - 0.5)*log(alpha) - alpha + 0.5*log(2*pi) )
	temp2 <-  alpha*log(beta) - (alpha + 1)*log(x) - beta/x
	exp( temp1 + temp2 )
}
### post.rho := p(rho | sigma^2, y) ###
post.rho <- function(x) dnorm( x, mu, sigma / sqrt( sumB ) ) 
### Monte Carlo ###
S <- 5000
sig.mc <- rigamma(S, alpha, beta)
rho.mc <- rep(NA, S)
mu <- sumA / sumB
for(i in 1:S)	{
	tau <- sqrt( sig.mc[i] / sumB )
	rho.mc[i] <- rnorm( n = 1, mean = mu, sd = tau )
}
### resumenes ###
( alpha )
( beta )
( mu )
### sigma^2 ###
( sig.me <- mean( sig.mc ) )
( sig.sd <- sd( sig.mc ) )
( sig.CI <- quantile( sig.mc, probs=c(0.025,0.975) ) )
### theta ###
( rho.me <- mean( rho.mc ) )
( rho.sd <- sd( rho.mc ) )
( rho.CI <- quantile( rho.mc, probs=c(0.025,0.975) ) )
### plot 1: posterior distr. sigma^2 | y ###
hist(sig.mc, freq=F, col = "gray99", border = "gray48",
  	las=1, cex.axis=0.75, xlim=c(3.0, 5.5), ylim=c(0, 1.6),
	xlab=expression(sigma^2), sub="Caso 1",
	main=expression( paste( 'p(' , sigma^2 , '|y)' ) )
	)
	curve(post.sigma, 3.0, 5.5, col="red", 
	las=1, lwd=1, n=1000, add=TRUE)
abline(v=mean(sig.mc), lty = 3, col="black", lwd=2)
### plot 2: posterior distr. rho | sigma^2 y ###
hist(rho.mc, freq=F, col = "gray99", border = "gray48",
  	las=1, cex.axis=0.75, 	xlab=expression(rho), sub="Caso 1",
	main=expression( paste( 'p(' , rho , '|',sigma^2,'y)' ) ),
	ylim=c(0,33) )
	curve(post.rho, 0.92, 1.02, col="blue", 
	las=1, lwd=1, n=1000, add=TRUE)
abline(v=mean(rho.mc), lty = 3, col="black", lwd=2)
```

```{r}
###---------###
# Escenario 2 #
###--=------###
### parametros iniciales ###
n <- 500
rho <- 0.3
sigma <- 2 
time <- 1:n
###--###
# Data #
###--###
set.seed(123456789)
y <- data(n, rho, sigma)
plot(time, y, type="l", xlab="t", ylab=expression(y[t]), las=1,
     cex.axis=0.75, main="Caso 2",
     sub = ' ')
```

```{r}
###--------------------###
# Posterior distribution #
###--------------------###
### sumA := sum_{t=2}^n y_t y_{t-1}^2 ###
sumA <- sum( y[-c(1)] * y[-c(n)] )
### sumB := sum_{t=2}^n y_{t-1}^2 ###
sumB <- sum( (y[-c(n)])^2 )
### sumC := sum_{t=2}^n y_t^2 ###
sumC <- sum( (y[-c(1)])^2 )
### paramteros posterior ###
alpha <- n/2 - 1
beta <- ( sumB * sumC - sumA^2 ) / ( 2 * sumB )
### post.sigma := p(sigma^2 | y) ###
require(pscl)
post.sigma <- function(x){
	### aproximacion funcion gamma ###
	temp1 <- - ( (alpha - 0.5)*log(alpha) - alpha + 0.5*log(2*pi) )
	temp2 <-  alpha*log(beta) - (alpha + 1)*log(x) - beta/x
	exp( temp1 + temp2 )
}
### post.rho := p(rho | sigma^2, y) ###
post.rho <- function(x) dnorm( x, mu, sigma / sqrt( sumB ) ) 
### Monte Carlo ###
S <- 5000
sig.mc <- rigamma(S, alpha, beta)
rho.mc <- rep(NA, S)
mu <- sumA / sumB
for(i in 1:S)	{
	tau <- sqrt( sig.mc[i] / sumB )
	rho.mc[i] <- rnorm( n = 1, mean = mu, sd = tau )
}
### Posterior ###
( alpha )
( beta )
( mu )
### sigma^2 ###
( sig.me <- mean( sig.mc ) )
( sig.sd <- sd( sig.mc ) )
( sig.CI <- quantile( sig.mc, probs=c(0.025,0.975) ) )
### theta ###
( rho.me <- mean( rho.mc ) )
( rho.sd <- sd( rho.mc ) )
( rho.CI <- quantile( rho.mc, probs=c(0.025,0.975) ) )
### plots ###
### plot 1: posterior distr. sigma^2 | y ###
hist(sig.mc, freq=F, col = "gray99", border = "gray48",
  	las=1, cex.axis=0.75, xlim=c(3.0, 5.5), ylim=c(0, 1.6),
	xlab=expression(sigma^2), sub="Caso 2",
	main=expression( paste( 'p(' , sigma^2 , '|y)' ) )
	)
	curve(post.sigma, 3.0, 5.5, col="red", 
	las=1, lwd=1, n=1000, add=TRUE)
abline(v=mean(sig.mc), lty = 3, col="black", lwd=2)
### plot 2: posterior distr. rho | sigma^2 y ###
hist(rho.mc, freq=F, col = "gray99", border = "gray48",
  	las=1, cex.axis=0.75, ylim=c(0.0, 10),
	xlab=expression(rho), sub="Caso 2",
	main=expression( paste( 'p(' , rho , '|',sigma^2,'y)' ) )
	)
	curve(post.rho, 0.20, 0.50, col="blue", 
	las=1, lwd=1, n=1000, add=TRUE)
abline(v=mean(rho.mc), lty = 3, col="black", lwd=2)
```




6. Replicar el caso de estudio acerca de los puntajes de matemáticas de los estudiantes que presentaron la Prueba Saber 11 para el segundo semestre de 2020 (los datos se encuentran disponibles en `SB11_2.txt`), ajustando un modelo Normal jerárquico con medias especificas y varianza común, y también un modelo Normal jerárquico con medias y varianzas especificas.

Ver las notas de clase.

